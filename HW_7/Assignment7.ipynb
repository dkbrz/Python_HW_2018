{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация названий групп\n",
    "\n",
    "Данные\n",
    "\n",
    "1. Датасет с именами групп\n",
    "2. Фильтруем те, что хотя бы начинаются с ascii символов, чтобы не попадал бред (там мало другого, ничего все равно не выйдет)\n",
    "3. Данные нужны только как обучение нграмм, поэтому по сути это строчки, нам не важно их делить по группам, просто перемешать, тогда переносы строк мы сделаем аналогом eos и потом учтем при генерации\n",
    "4. Для удобства возьмем только латиницу, пробел и перенос строки (конец слова)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch as tt\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./death-metal/bands.csv')\n",
    "df[df['name']<= 'Z'][['name']].to_csv('bands.txt', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bands.txt') as f:\n",
    "    names = f.readlines()\n",
    "    random.shuffle(names)\n",
    "    names = ''.join(names)\n",
    "train = names[:int(len(names)*0.7)]\n",
    "valid = names[int(len(names)*0.7):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_characters = ' '+'\\n'+string.ascii_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tensor(string):\n",
    "    tensor = tt.zeros(len(string)).long()\n",
    "    for ci in range(len(string)):\n",
    "        try:\n",
    "            tensor[ci] = all_characters.index(string[ci])\n",
    "        except:\n",
    "            pass\n",
    "    return tensor\n",
    "\n",
    "def random_training_set(chunk_len, batch_size, file):\n",
    "    limit = len(file) - chunk_len\n",
    "    inp = tt.LongTensor(batch_size, chunk_len)\n",
    "    target = tt.LongTensor(batch_size, chunk_len)\n",
    "    \n",
    "    for bi in range(batch_size):\n",
    "        start_index = random.randint(0, limit)\n",
    "        chunk = file[start_index : start_index + chunk_len + 1]\n",
    "        inp[bi] = char_tensor(chunk[:-1])\n",
    "        target[bi] = char_tensor(chunk[1:])\n",
    "    \n",
    "    return inp, target\n",
    "\n",
    "def perplexity(x):\n",
    "    return 2**x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_epoch(inp, target, model, optimizer, criterion, curr_epoch):\n",
    "\n",
    "    decoder.train()\n",
    "    hidden = decoder.init_hidden(batch_size)\n",
    "    decoder.zero_grad()\n",
    "    \n",
    "    train_loss = 0\n",
    "    perplexities = []\n",
    "    \n",
    "    for ci in range(chunk_len):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, hidden = decoder(inp[:,ci], hidden)\n",
    "        loss = criterion(output.view(batch_size, -1), target[:,ci])\n",
    "        perplexities.append(perplexity(loss.item()))\n",
    "        \n",
    "        current_loss = loss.data.cpu().detach().item()\n",
    "        loss_smoothing = ci / (ci+1)\n",
    "        train_loss = loss_smoothing * train_loss + (1 - loss_smoothing) * current_loss\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    result_perplexity = np.mean(perplexities)\n",
    "    return train_loss, result_perplexity\n",
    "\n",
    "def _test_epoch(inp, target, model, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    loss = 0\n",
    "    perplexities = []\n",
    "    \n",
    "    hidden = decoder.init_hidden(batch_size)\n",
    "    \n",
    "    with tt.no_grad():\n",
    "        for ci in range(chunk_len):\n",
    "            output, hidden = decoder(inp[:,ci], hidden)\n",
    "            loss = criterion(output.view(batch_size, -1), target[:,ci])\n",
    "            perplexities.append(perplexity(loss.item()))\n",
    "            epoch_loss += loss.data.item()\n",
    "    \n",
    "    result_perplexity = np.mean(perplexities)\n",
    "    return epoch_loss / chunk_len, result_perplexity\n",
    "\n",
    "\n",
    "def nn_train(model, train, valid, criterion, optimizer, n_epochs=100, scheduler=None, early_stopping=0):\n",
    "    \n",
    "    print('EPOCH\\tValid Loss\\t Train Loss\\tV.Perplexity\\tT.Perplexity')\n",
    "    \n",
    "    best_epoch = None\n",
    "    prev_loss = 100500\n",
    "    es_epochs = 0\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        try:\n",
    "            train_loss, train_per = _train_epoch(*random_training_set(chunk_len, \n",
    "                                                                      batch_size, \n",
    "                                                                      train),\n",
    "                                                 model, optimizer, criterion, epoch)\n",
    "            valid_loss, valid_per = _test_epoch(*random_training_set(chunk_len, \n",
    "                                                                     batch_size, \n",
    "                                                                     valid),\n",
    "                                                model, criterion)\n",
    "            train_losses.append(train_loss)\n",
    "            valid_losses.append(valid_loss)\n",
    "            \n",
    "            if epoch % 100 == 0 or epoch == n_epochs-1:\n",
    "                print('%s \\t %.5f \\t %.5f \\t %.5f \\t %.5f' % (str(epoch),\n",
    "                                                                 valid_loss,\n",
    "                                                                 train_loss,\n",
    "                                                                 valid_per,\n",
    "                                                                 train_per))\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "        \n",
    "        if early_stopping > 0:\n",
    "            if valid_loss > prev_loss:\n",
    "                es_epochs += 1\n",
    "            else:\n",
    "                es_epochs = 0\n",
    "            if es_epochs >= early_stopping:\n",
    "                break\n",
    "            prev_loss = min(prev_loss, valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.GRU(hidden_size, hidden_size, 1)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        batch_size = input.size(0)\n",
    "        \n",
    "        embed = self.encoder(input)\n",
    "        output, hidden = self.rnn(embed.view(1, batch_size, -1), hidden)\n",
    "        output = self.decoder(output.view(batch_size, -1))\n",
    "        \n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return tt.zeros(1, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "chunk_len = 256 # побольше\n",
    "\n",
    "decoder = MyModel(input_size=len(all_characters),\n",
    "                  hidden_size=hidden_size, \n",
    "                  output_size=len(all_characters))\n",
    "\n",
    "optimizer = tt.optim.Adam(decoder.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH\tValid Loss\t Train Loss\tV.Perplexity\tT.Perplexity\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d02e4693bd4d779865bfc3e0f7c2b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t 3.92507 \t 4.00702 \t 15.21317 \t 16.08188\n",
      "100 \t 2.77380 \t 2.73309 \t 6.94665 \t 6.73758\n",
      "200 \t 2.80735 \t 2.79752 \t 7.14937 \t 7.07524\n",
      "300 \t 2.75402 \t 2.75679 \t 6.87707 \t 6.86114\n",
      "400 \t 2.74931 \t 2.73051 \t 6.87575 \t 6.77160\n",
      "500 \t 2.74841 \t 2.74662 \t 6.83037 \t 6.83360\n",
      "600 \t 2.75724 \t 2.77432 \t 6.91470 \t 6.97971\n",
      "700 \t 2.77977 \t 2.83520 \t 7.00832 \t 7.27761\n",
      "800 \t 2.73841 \t 2.71998 \t 6.80859 \t 6.71167\n",
      "900 \t 2.76704 \t 2.73545 \t 6.94108 \t 6.79947\n",
      "999 \t 2.78648 \t 2.74800 \t 7.03559 \t 6.83316\n"
     ]
    }
   ],
   "source": [
    "nn_train(decoder, train, valid, criterion, optimizer, n_epochs=1000, early_stopping=500)\n",
    "tt.save(decoder, 'model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity ок, если выше, то рандом, если ниже, то это повторение симовола (по опытам)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(decoder, prime_str='\\n', predict_len=30, temperature=0.8):\n",
    "    hidden = decoder.init_hidden(1)\n",
    "    prime_input = char_tensor(prime_str).unsqueeze(0)\n",
    "    predicted = ''\n",
    "\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[:,p], hidden)\n",
    "        \n",
    "    inp = prime_input[:,-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = tt.multinomial(output_dist, 1)[0]\n",
    "        predicted_char = all_characters[top_i]\n",
    "        \n",
    "        if predicted and predicted_char == '\\n':\n",
    "            break\n",
    "        else:\n",
    "            predicted += predicted_char\n",
    "            inp = char_tensor(predicted_char).unsqueeze(0)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chamk Inficunorgt\n",
      "Suecnar\n",
      "Strrencion\n",
      "Inficen\n",
      "Canoum Pras\n",
      "Cozboupption\n",
      "Missianc Incher\n",
      "Missum of of Bl\n",
      "Bordloat Mostic Por\n",
      "Tepoc Disecy\n",
      "Bacionc Egof Anesthion\n",
      "Gouition\n",
      "Amusssked\n",
      "Exinificif Tock Desgo\n",
      "Algost of Arcrication\n",
      "Poud Deass\n",
      "Goulustures\n",
      "Ey Fedess\n",
      "Agospcfg the Flom\n",
      "Infiing Flosos Votitio\n",
      "Tearpoust\n",
      "Comed\n",
      "Exisy of Posume\n",
      "Shuredes\n",
      "CArdopevespicatos\n",
      "Drume\n",
      "Exibilas\n",
      "Carpys\n",
      "Exic Revico\n",
      "Ensiont oum\n",
      "Magare Frosss\n",
      "Decal geh Ety Oncifat of Dyssi\n",
      "Stringrurl\n",
      "Inulim of\n",
      "Meassulefifede Stion\n",
      "CMonsippct Moul Fopt\n",
      "Exicicastion\n",
      "Voct Dead\n",
      "Gehadod Posy\n",
      "Kcom\n",
      "Exilinck Flaifg of Py\n",
      "Lerifi\n",
      "Infiuminatic Flosos\n",
      "Ipegocite\n",
      "Morgiecatifion\n",
      "Fo the Flopsosti\n",
      "Te Totoruise\n",
      "Verostiing Pod Dokitis\n",
      "Kumsurcifospom\n",
      "Gazul\n",
      "Slceddcifer\n",
      "Cacogustic sot Le\n",
      "Migncifont\n",
      "Exicues Tot\n",
      "Aboss\n",
      "GHod Infcrimeo\n",
      "I Tokmopes\n",
      "Honpe Srne\n",
      "GoulArs\n",
      "Mostruxiibiost\n",
      "Exillerd\n",
      "Totuer\n",
      "In Trof Posephero\n",
      "Pumelon\n",
      "Gorus\n",
      "Te Toks cetul\n",
      "Gised\n",
      "Neroust\n",
      "Inficathionto\n",
      "Inifondfipipuried\n",
      "Totormpe thit\n",
      "Ming Cackess\n",
      "Leagninifine Pos Noos\n",
      "Lead Gry Day\n",
      "Schurdred\n",
      "Exicaty\n",
      "Cavivotigrem\n",
      "Tecor\n",
      "Neadesit Pockry\n",
      "Cousic Pouss tof the Trode\n",
      "Toreack sot\n",
      "Enacidcess Poosy\n",
      "Te of Deasys\n",
      "Goked\n",
      "Ecar Cral\n",
      "Hom Der\n",
      "Cosousages\n",
      "Eys f Inrifininicifithom\n",
      "Nosstion Possianic Ner Inic In\n",
      "Thiniros\n",
      "Tocksned\n",
      "Mogofic Codity\n",
      "Teposs\n",
      "CSaburemiing Son Molity Tion\n",
      "Ktumbigzol\n",
      "Ecacarion\n",
      "Cellonicoustity\n",
      "Anocsoncar tor\n",
      "Stru nonity Enifiefacros\n",
      "ColllAbad\n"
     ]
    }
   ],
   "source": [
    "for x in range(100):\n",
    "    print(generate(decoder))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
